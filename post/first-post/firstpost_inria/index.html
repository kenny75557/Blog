<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog的第一篇，從2022年實習寫的論文開始好了 | Heliart's Blog</title><meta name=keywords content="ACL2023,Paper,Intern"><meta name=description content="開始即結束？"><meta name=author content><link rel=canonical href=http://example.org/post/first-post/firstpost_inria/><link crossorigin=anonymous href=/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=http://example.org/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://example.org/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://example.org/favicon-32x32.png><link rel=apple-touch-icon href=http://example.org/apple-touch-icon.png><link rel=mask-icon href=http://example.org/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Blog的第一篇，從2022年實習寫的論文開始好了"><meta property="og:description" content="開始即結束？"><meta property="og:type" content="article"><meta property="og:url" content="http://example.org/post/first-post/firstpost_inria/"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-06-27T17:23:48+08:00"><meta property="article:modified_time" content="2023-06-27T17:23:48+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Blog的第一篇，從2022年實習寫的論文開始好了"><meta name=twitter:description content="開始即結束？"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://example.org/post/"},{"@type":"ListItem","position":2,"name":"Blog的第一篇，從2022年實習寫的論文開始好了","item":"http://example.org/post/first-post/firstpost_inria/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Blog的第一篇，從2022年實習寫的論文開始好了","name":"Blog的第一篇，從2022年實習寫的論文開始好了","description":"開始即結束？","keywords":["ACL2023","Paper","Intern"],"articleBody":" 前言: Remote的海外實習之旅🇫🇷 21年11月的某天，這天是個星期二，我一如往常在lab刷arxiv找論文，尋求一絲下週Meeting不會被汎老師釘在牆上的希望; 隨著我打算放棄稍作休息時，看到了老師在社團分享的實習資訊。 嗯…基於好奇心提問的問題生成看起來是教育相關應用，NLP的部分想要引入GPT-3來做題目生成及Evaluation吧？\n再看一下單位： Inria Bordeaux 跟 MSR Montreal的合作，Microsoft Research很難不知道，不過Inria就需要查一下了。\nInria，全名Institut National de Recherche en Informatique et en Automatique（法國國家信息與自動化研究所)，順便發現Scikit Learn是他家開發的。 雖然我的碩論方向不是做QG也不是Prompt Engineering，但實習可以去法國半年，本人也不急著畢業找工作什麼的，於是種種誘因下我準備了履歷，麻煩老師幫我潤一下Cover letter之後就發射過去了。 那過程就是三場英文面試，ㄧ場講了做過的NLP專案另外兩場聊天居多，沒想到順利的上了XD\n題外話，雖然專案是Inria那邊Host，但後來我幾乎都跟Montreal那邊的Team做LLM相關的內容。 Mentor也覺得我的情況做NLP比起去系統端更合適就邀我過去(畢竟學校這邊在寫碩論)，然後提出了組內會議的時候可以講中文，原因我不好說… 如標題所說後來因疫情關係採遠端實習，所以變成一邊做碩論，口試時間也不用延後，外加他們會招待我一趟7天的參訪; 打聽了下，他們能給的補助也無法負擔在那邊的生活費，所以有送機票住宿還能準時畢業算是一個因禍得福的Solution吧。 至於我後來決定把波爾多一週參訪變成一個月的法國之旅又是另一個故事了。\n廢話講完了進下一Part論文講解～\nSelecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation 📖 Introduction: 應用GPT-3生成引導學童提問系統所需的問題（Question Generation）時，當要求LLM生成多個問題，後面很容易回相同的問題或是太General的Question(ex:結局是歡樂的嗎?) 最終目的是全自動化出題，除了優化Prompt及調Temperature參數外，還可以設計方法在這些LLM outputs中Sample出好的答案。\n註：要提升LM的生成多樣性(Generation Divesity)常見的方法就是Sampling ，關於LM Sampling，我們之後會專門做一集視頻跟大家講解。\n先前沒有穩定能從LLM的所有Output中sample最佳的方法，於是我們的方法就建立在以下背景：\n黑盒子模型 (non-modifiable 的 QG Model) 缺乏人工標注的資料 而上述設定其實就是在現實中部署LLM會遇到的限制，只是每個Industry的需求問題不一樣。 我們設計了自動評估及利用Human Evaluation，來驗證所提出的方法所挑選出來的問題是否比Greedy策略（LLM的第一筆生成結果，即最大機率）生成的結果更理想。\nProblem Setting 跟一般Question Generation 相同，Dataset中的Context-Answer Pairs (c,a)都是字串，這個Task是要讓模型生成一個對應到答案 a 的問題 q，同時這個問題必須用Context c作為supporting evidence。\n不同的地方來了，在先前提到的設定中，對每一組Context-Answer我們會Sample k個問題，對照組則是Greedy生成的一個回答 (很抽象？其實就是調Temperature參數而已)。\n所以到這邊每篇Context我們會有k個 sample questions跟一個Greedy的question；我們將k個samples的平均分數以及Greedy的分數作為兩個Baseline (算分方式後面會介紹) 理想情況下實驗結果應該要優於這兩個Baseline。 Notion Define\nDataset \u0026 Model 資料集：\nSQuAD : Sentence Level, ExtractiveQA. Fairy-tail QA : Paragraph Level, Abstractive QA. Zero-Shot setting，都只使用TestingData。 模型：\nGPT-3 text-davinci-003\n設定及Prompt可參考論文\nEvaluation Metrics 我們使用兩個metrics來評估選出的q’\nReference-based evaluation 沿用先前相關研究的指標，SQuAD 用的是 BLUE-4，FairyTale QA 用的是ROUGE-L。 Human evaluation 單純要標註者評估問題好壞肯定是高度主觀的，因此與Inria那邊共同設計了7個dimension跟一個Overall分數，我們稱為Meta Question，讓標註者進行評分，每一組資料至少有三位Anotator。 Method 設計三種手法，兩個Reference\nn-gram similarity:\n用n-gram來衡量Context與生出question的關聯性，這個手法較為直接，Assume與Context資訊重疊多的為品質好的問題，i.e., 這篇文章的標題應該是什麼，這類通用問句的Similarity score就會較低 RoundTrip:\n生成出的問題對應到的答案應該要與資料提供給生成的答案相同。也就是理想的情況下QAQG model的輸出應符合: ，此想法近似於 Cycle consistency在圖片生成及翻譯上的應用。\n實作的話，將GPT-3生成的 Q 與 Context 重新輸入得到答案，與資料集QA pair中的 Answer 計算F1Score (採用與資料集Evaluation setup相同的BLUE-4, ROUGE)，最後sample能讓GPT-3生成出最高分答案的Q。 Prompt-based Score:\nInria另一篇研究需要針對題目的品質，找Annotater做不同面向的評估，設計了七個維度的量表，包含文法正確性,與文章內容相不相關等等，於是直接拿了這個量表來反問GPT-3，一個自評生成的好不好的概念。 分為兩步驟:\nStep1. 將文本與Sample出的問題丟給GPT-3，詢問上面提到的Meta-Question，並要Model給出理由。 Step2. 讓Model based on 問題, 文本, 以及生成的理由，回答一個分數（其實就是做跟Human Annotater相同的事） 這麼做的原因是我們觀察到如果沒有Step1要model給出理由，GPT-3傾向Low-entropy 的 Distribution，就是說同一個Meta-Question模型傾向於選擇同樣的分數，而不考慮Context-Question pair。 至於為何加上的先詢問理由的步驟就會比較好，個人認為大部分的原因還是通靈，我覺得這邊的概念有點像CoT(Chain of Thought)，只是我們當初沒有設計一套評分準則跟推理過程而是讓 GPT-3 自己Inference。\n最後，方法中使用的OPS Score及APS Score分別代表：\noverall prompt-based score: 來自人工標注中其中一題，請給這個問題一個Overall score。也同時問了GPT-3，挑出最高的。 averaged promptbased score: 七個Meta Question的平均分數最高的。 實驗 隨機從GPT-3 Sample的k個Question中使用設計的Selection Method來挑出Question，使用前面提到的兩種Evaluation評估。\n此外也做了多種method的Ensemble performance，將Reference-based及Human Evaluation的分數都Normalize到0-1之間以確保比較性。\nReference-based evaluation: 首先看到在SQuAD所有的selction method是高於Average Score(5個Sample questions平均)的，而各個selction methods也都優於Greedy generation的Baseline。 要注意的是先前的比較對象都是經過Label data的Fine-tuning，而我們用的是Zero-shot GPT-3。\n在Fairy-Tale只有Upperbound超過BART的Performance，在Abstractive QG上，Selection method還有蠻多空間。\nHuman Evaluation: 在QG以及其他NLG task，LM輸出的內容會是具有語言多樣性(linguistic diversity)但皆有相同語意(Semantic Equivalance)，因此使用Single Reference的Evaluation是不合適的(Ground Truth)。實驗中也發現，GT的分數在大部分情況下都不是人為評估的最佳。 因此我們認為Human Evalution是必要且對提高Performance泛用到實際應用是有幫助的。\n在實驗中，APS以及n-gram的Selection Method在兩個資料集表現有較明顯的差異，在SQuAD資料集，n-gram如同前面Reference based一樣表現得較好，加上RoundTrip的Ensemble method甚至可以更高，APS則是在這些meta-question表現得最差。\n相反的Fairy-Tale資料集上n-gram表現的最差，APS則是明顯的最優。 會有這樣的現象我們認為是SQuAD資料集的特性，也就是他的答案長度較短且都是從較短的Context中Extract出來的，因此參考上下文關聯性的N-gram在這種資料下就會是一個好的Question Selection Method。\n另一方面並不是所有需要問提生成的應用都是在這樣的出題模式(多段文章, abstractive) ，只能說若不在短文且答案在文章中的Scenario，那麼n-gram不是一個通用且好的Question Selection strategy。\nDiscussion\n後記 回到現在，在前幾個月經歷了:\n當個兵出來發現論文上ACL (都跟朋友說我是：A real imposter, no syndrome.)、\n結訓出來求職（時機歹歹的科技業寒冬，過程有frustrated到；但不少機會沒能拿下確實是自己玩砸了Hehe）、\n訂完去多倫多的機票隔一小時拿到Offer\n後寫了這段發現半年過去，體感超快…； 其實試著寫部落格掛在嘴邊很久了，但總是因為懶或是覺得自己沒料（確實）而Pending; 一個月前跟朋友討論後覺得以這個主題當第一篇，這樣去研討會的時候就可以日更，但拖延症大家都老熟了XDD\n寫這篇的時間是23年7月4日的凌晨，明晚就要出發，還好趕上發布，那今天就先寫到這了咱們下篇再見。\n","wordCount":"241","inLanguage":"en","datePublished":"2023-06-27T17:23:48+08:00","dateModified":"2023-06-27T17:23:48+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.org/post/first-post/firstpost_inria/"},"publisher":{"@type":"Organization","name":"Heliart's Blog","logo":{"@type":"ImageObject","url":"http://example.org/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://example.org/ accesskey=h title="Heliart's Blog (Alt + H)">Heliart's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://example.org/categories/ title=Categories><span>categories</span></a></li><li><a href=http://example.org/tags/ title=Tags><span>tags</span></a></li><li><a href=http://example.org/archives/ title=archives><span>archives</span></a></li><li><a href=http://example.org/about/ title="About me"><span>About me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://example.org/>Home</a>&nbsp;»&nbsp;<a href=http://example.org/post/>Posts</a></div><h1 class=post-title>Blog的第一篇，從2022年實習寫的論文開始好了</h1><div class=post-description>開始即結束？</div><div class=post-meta><span title='2023-06-27 17:23:48 +0800 +0800'>June 27, 2023</span>&nbsp;·&nbsp;2 min</div></header><div class=post-content><p><img loading=lazy src=/post/first-post/inria_BOD.jpg alt="INRIA Bordeaux" title="INRIA Bordeaux"></p><h3 id=前言-remote的海外實習之旅>前言: Remote的海外實習之旅🇫🇷<a hidden class=anchor aria-hidden=true href=#前言-remote的海外實習之旅>#</a></h3><p>21年11月的某天，這天是個星期二，我一如往常在lab刷arxiv找論文，尋求一絲下週Meeting不會被汎老師釘在牆上的希望; 隨著我打算放棄稍作休息時，看到了老師在社團分享的實習資訊。
<img loading=lazy src=/post/first-post/InternProject.png alt="Intern Info." title="Intern Info."></p><p>嗯&mldr;基於好奇心提問的問題生成看起來是教育相關應用，NLP的部分想要引入GPT-3來做題目生成及Evaluation吧？</p><p>再看一下單位：
Inria Bordeaux 跟 MSR Montreal的合作，Microsoft Research很難不知道，不過Inria就需要查一下了。</p><p>Inria，全名Institut National de Recherche en Informatique et en Automatique（法國國家信息與自動化研究所)，順便發現Scikit Learn是他家開發的。
雖然我的碩論方向不是做QG也不是Prompt Engineering，但實習可以去法國半年，本人也不急著畢業找工作什麼的，於是種種誘因下我準備了履歷，麻煩老師幫我潤一下Cover letter之後就發射過去了。
那過程就是三場英文面試，ㄧ場講了做過的NLP專案另外兩場聊天居多，沒想到順利的上了XD</p><p>題外話，雖然專案是Inria那邊Host，但後來我幾乎都跟Montreal那邊的Team做LLM相關的內容。 Mentor也覺得我的情況做NLP比起去系統端更合適就邀我過去(畢竟學校這邊在寫碩論)，然後提出了組內會議的時候可以講中文，原因我不好說&mldr;
<img loading=lazy src=/post/first-post/ghost.jpeg alt="阿鬼 你還是說中文吧" title="阿鬼 你還是說中文吧"></p><p>如標題所說後來因疫情關係採遠端實習，所以變成一邊做碩論，口試時間也不用延後，外加他們會招待我一趟7天的參訪;
打聽了下，他們能給的補助也無法負擔在那邊的生活費，所以有送機票住宿還能準時畢業算是一個因禍得福的Solution吧。
至於我後來決定把波爾多一週參訪變成一個月的法國之旅又是另一個故事了。</p><p>廢話講完了進下一Part論文講解～</p><h3 id=selecting-better-samples-from-pre-trained-llms-a-case-study-on-question-generation->Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation 📖<a hidden class=anchor aria-hidden=true href=#selecting-better-samples-from-pre-trained-llms-a-case-study-on-question-generation->#</a></h3><h5 id=introduction><strong>Introduction:</strong><a hidden class=anchor aria-hidden=true href=#introduction>#</a></h5><p>應用GPT-3生成引導學童提問系統所需的問題（Question Generation）時，當要求LLM生成多個問題，後面很容易回相同的問題或是太General的Question(ex:結局是歡樂的嗎?)
最終目的是全自動化出題，除了優化Prompt及調Temperature參數外，還可以設計方法在這些LLM outputs中Sample出好的答案。</p><p>註：要提升LM的生成多樣性(Generation Divesity)常見的方法就是Sampling <del>，關於LM Sampling，我們之後會專門做一集視頻跟大家講解。</del></p><p>先前沒有穩定能從LLM的所有Output中sample最佳的方法，於是我們的方法就建立在以下背景：</p><ol><li>黑盒子模型 (non-modifiable 的 QG Model)</li><li>缺乏人工標注的資料</li></ol><p>而上述設定其實就是在現實中部署LLM會遇到的限制，只是每個Industry的需求問題不一樣。
我們設計了自動評估及利用Human Evaluation，來驗證所提出的方法所挑選出來的問題是否比Greedy策略（LLM的第一筆生成結果，即最大機率）生成的結果更理想。</p><h5 id=problem-setting><strong>Problem Setting</strong><a hidden class=anchor aria-hidden=true href=#problem-setting>#</a></h5><p>跟一般Question Generation 相同，Dataset中的Context-Answer Pairs (c,a)都是字串，這個Task是要讓模型生成一個對應到答案 <strong>a</strong> 的問題 <strong>q</strong>，同時這個問題必須用Context <strong>c</strong>作為supporting evidence。</p><p>不同的地方來了，在先前提到的設定中，對每一組Context-Answer我們會Sample <strong>k</strong>個問題，對照組則是Greedy生成的一個回答
(很抽象？其實就是調Temperature參數而已)。</p><p>所以到這邊每篇Context我們會有k個 sample questions跟一個Greedy的question；我們將k個samples的平均分數以及Greedy的分數作為兩個Baseline (算分方式後面會介紹)
理想情況下實驗結果應該要優於這兩個Baseline。
<img loading=lazy src=/post/first-post/Baseline.jpg alt=Baseline title=Baseline>
<a href=https://arxiv.org/pdf/2209.11000.pdf>Notion Define</a></p><h5 id=dataset--model><strong>Dataset & Model</strong><a hidden class=anchor aria-hidden=true href=#dataset--model>#</a></h5><p>資料集：</p><ol><li>SQuAD : Sentence Level, ExtractiveQA.</li><li>Fairy-tail QA : Paragraph Level, Abstractive QA.
Zero-Shot setting，都只使用TestingData。</li></ol><p>模型：</p><p>GPT-3 text-davinci-003</p><p><a href=https://arxiv.org/pdf/2209.11000.pdf>設定及Prompt可參考論文</a></p><h5 id=evaluation-metrics><strong>Evaluation Metrics</strong><a hidden class=anchor aria-hidden=true href=#evaluation-metrics>#</a></h5><p>我們使用兩個metrics來評估選出的<strong>q&rsquo;</strong></p><ol><li>Reference-based evaluation<ul><li>沿用先前相關研究的指標，SQuAD 用的是 BLUE-4，FairyTale QA 用的是ROUGE-L。</li></ul></li><li>Human evaluation<ul><li>單純要標註者評估問題好壞肯定是高度主觀的，因此與Inria那邊共同設計了7個dimension跟一個Overall分數，我們稱為Meta Question，讓標註者進行評分，每一組資料至少有三位Anotator。</li></ul></li></ol><p><img loading=lazy src=/post/first-post/MetaQuestion.png alt=MetaQuestion title=MetaQuestion></p><h5 id=method><strong>Method</strong><a hidden class=anchor aria-hidden=true href=#method>#</a></h5><p>設計三種手法，兩個Reference</p><ol><li><p><strong>n-gram similarity:</strong></p><ul><li>用n-gram來衡量Context與生出question的關聯性，這個手法較為直接，Assume與Context資訊重疊多的為品質好的問題，i.e., 這篇文章的標題應該是什麼，這類通用問句的Similarity score就會較低</li></ul></li><li><p><strong>RoundTrip:</strong></p><ul><li>生成出的問題對應到的答案應該要與資料提供給生成的答案相同。也就是理想的情況下QAQG model的輸出應符合:</li></ul><p><img loading=lazy src=/post/first-post/RoundTrip.png alt=RoundTrip title=RoundTrip>
，此想法近似於 Cycle consistency在圖片生成及翻譯上的應用。</p><ul><li>實作的話，將GPT-3生成的 Q 與 Context 重新輸入得到答案，與資料集QA pair中的 Answer 計算F1Score (採用與資料集Evaluation setup相同的BLUE-4, ROUGE)，最後sample能讓GPT-3生成出最高分答案的Q。</li></ul></li><li><p><strong>Prompt-based Score:</strong></p><p>Inria另一篇研究需要針對題目的品質，找Annotater做不同面向的評估，設計了七個維度的量表，包含文法正確性,與文章內容相不相關等等，於是直接拿了這個量表來反問GPT-3，一個自評生成的好不好的概念。
分為兩步驟:</p><p><img loading=lazy src=/post/first-post/humanEval.png alt=humanEval title=humanEval></p><ul><li>Step1. 將文本與Sample出的問題丟給GPT-3，詢問上面提到的Meta-Question，並要Model給出理由。</li><li>Step2. 讓Model based on 問題, 文本, 以及生成的理由，回答一個分數（其實就是做跟Human Annotater相同的事）
這麼做的原因是我們觀察到如果沒有Step1要model給出理由，GPT-3傾向Low-entropy 的 Distribution，就是說同一個Meta-Question模型傾向於選擇同樣的分數，而不考慮Context-Question pair。</li></ul></li></ol><p>至於為何加上的先詢問理由的步驟就會比較好，<del>個人認為大部分的原因還是通靈</del>，我覺得這邊的概念有點像CoT(Chain of Thought)，只是我們當初沒有設計一套評分準則跟推理過程而是讓 GPT-3 自己Inference。</p><p>最後，方法中使用的OPS Score及APS Score分別代表：</p><ol><li>overall prompt-based score: 來自人工標注中其中一題，請給這個問題一個Overall score。也同時問了GPT-3，挑出最高的。</li><li>averaged promptbased score: 七個Meta Question的平均分數最高的。</li></ol><h5 id=實驗><strong>實驗</strong><a hidden class=anchor aria-hidden=true href=#實驗>#</a></h5><p>隨機從GPT-3 Sample的k個Question中使用設計的Selection Method來挑出Question，使用前面提到的兩種Evaluation評估。</p><p>此外也做了多種method的Ensemble performance，將Reference-based及Human Evaluation的分數都Normalize到0-1之間以確保比較性。</p><h6 id=reference-based-evaluation><strong>Reference-based evaluation:</strong><a hidden class=anchor aria-hidden=true href=#reference-based-evaluation>#</a></h6><p><img loading=lazy src=/post/first-post/Refbase_score.png alt=Refbase_score title=Refbase_score></p><p>首先看到在SQuAD所有的selction method是高於Average Score(5個Sample questions平均)的，而各個selction methods也都優於Greedy generation的Baseline。 要注意的是先前的比較對象都是經過Label data的Fine-tuning，而我們用的是Zero-shot GPT-3。</p><p>在Fairy-Tale只有Upperbound超過BART的Performance，在Abstractive QG上，Selection method還有蠻多空間。</p><h6 id=human-evaluation><strong>Human Evaluation:</strong><a hidden class=anchor aria-hidden=true href=#human-evaluation>#</a></h6><p><img loading=lazy src=/post/first-post/HEresults.png alt=HEresults title=HEresults></p><p>在QG以及其他NLG task，LM輸出的內容會是具有語言多樣性(linguistic diversity)但皆有相同語意(Semantic Equivalance)，因此使用Single Reference的Evaluation是不合適的(Ground Truth)。實驗中也發現，GT的分數在大部分情況下都不是人為評估的最佳。
因此我們認為Human Evalution是必要且對提高Performance泛用到實際應用是有幫助的。</p><p>在實驗中，APS以及n-gram的Selection Method在兩個資料集表現有較明顯的差異，在SQuAD資料集，n-gram如同前面Reference based一樣表現得較好，加上RoundTrip的Ensemble method甚至可以更高，APS則是在這些meta-question表現得最差。</p><p>相反的Fairy-Tale資料集上n-gram表現的最差，APS則是明顯的最優。
會有這樣的現象我們認為是SQuAD資料集的特性，也就是他的答案長度較短且都是從較短的Context中Extract出來的，因此參考上下文關聯性的N-gram在這種資料下就會是一個好的Question Selection Method。</p><p>另一方面並不是所有需要問提生成的應用都是在這樣的出題模式(多段文章, abstractive)
，只能說若不在短文且答案在文章中的Scenario，那麼n-gram不是一個通用且好的Question Selection strategy。</p><p><strong>Discussion</strong></p><h3 id=後記>後記<a hidden class=anchor aria-hidden=true href=#後記>#</a></h3><p>回到現在，在前幾個月經歷了:</p><ul><li><p>當個兵出來發現論文上ACL (都跟朋友說我是：A real imposter, no syndrome.)、</p></li><li><p>結訓出來求職（時機歹歹的科技業寒冬，過程有frustrated到；但不少機會沒能拿下確實是自己玩砸了Hehe）、</p></li><li><p>訂完去多倫多的機票隔一小時拿到Offer</p></li></ul><p>後寫了這段發現半年過去，體感超快&mldr;；
其實試著寫部落格掛在嘴邊很久了，但總是因為懶或是覺得自己沒料（確實）而Pending;
一個月前跟朋友討論後覺得以這個主題當第一篇，這樣去研討會的時候就可以日更，但拖延症大家都老熟了XDD</p><p>寫這篇的時間是23年7月4日的凌晨，明晚就要出發，還好趕上發布，那今天就先寫到這了咱們下篇再見。</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://example.org/tags/acl2023/>ACL2023</a></li><li><a href=http://example.org/tags/paper/>Paper</a></li><li><a href=http://example.org/tags/intern/>Intern</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Blog的第一篇，從2022年實習寫的論文開始好了 on twitter" href="https://twitter.com/intent/tweet/?text=Blog%e7%9a%84%e7%ac%ac%e4%b8%80%e7%af%87%ef%bc%8c%e5%be%9e2022%e5%b9%b4%e5%af%a6%e7%bf%92%e5%af%ab%e7%9a%84%e8%ab%96%e6%96%87%e9%96%8b%e5%a7%8b%e5%a5%bd%e4%ba%86&url=http%3a%2f%2fexample.org%2fpost%2ffirst-post%2ffirstpost_inria%2f&hashtags=ACL2023%2cPaper%2cIntern"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Blog的第一篇，從2022年實習寫的論文開始好了 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=http%3a%2f%2fexample.org%2fpost%2ffirst-post%2ffirstpost_inria%2f&title=Blog%e7%9a%84%e7%ac%ac%e4%b8%80%e7%af%87%ef%bc%8c%e5%be%9e2022%e5%b9%b4%e5%af%a6%e7%bf%92%e5%af%ab%e7%9a%84%e8%ab%96%e6%96%87%e9%96%8b%e5%a7%8b%e5%a5%bd%e4%ba%86&summary=Blog%e7%9a%84%e7%ac%ac%e4%b8%80%e7%af%87%ef%bc%8c%e5%be%9e2022%e5%b9%b4%e5%af%a6%e7%bf%92%e5%af%ab%e7%9a%84%e8%ab%96%e6%96%87%e9%96%8b%e5%a7%8b%e5%a5%bd%e4%ba%86&source=http%3a%2f%2fexample.org%2fpost%2ffirst-post%2ffirstpost_inria%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Blog的第一篇，從2022年實習寫的論文開始好了 on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2fexample.org%2fpost%2ffirst-post%2ffirstpost_inria%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=http://example.org/>Heliart's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>